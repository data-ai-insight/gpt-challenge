import streamlit as st
import os
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.storage import LocalFileStore
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

def process_file(file):
    """
    íŒŒì¼ì„ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ ì €ì¥
    file_content = file.read()
    file_path = f"./uploaded_files/{file.name}"
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    loader = UnstructuredFileLoader(file_path)
    splitter = CharacterTextSplitter.from_tiktoken_encoder(separator="\n", chunk_size=600, chunk_overlap=100)
    docs = loader.load_and_split(text_splitter=splitter)

    # ì„ë² ë”© ë° ìºì‹±
    cache_dir = LocalFileStore(f"./.cache/{file.name}")
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

def setup_chain(retriever, api_key):
    """
    Conversational Retrieval Chain ì„¤ì •.
    """
    # OpenAI LLM ì„¤ì •
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, openai_api_key=api_key)

    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True, 
        output_key="answer"
    )

    # ì²´ì¸ ìƒì„±
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True
    )
    return chain

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG Pipeline",
    page_icon="ğŸ“„",
    layout="wide"
)

# ì œëª© ë° ì„¤ëª…
st.title("RAG Pipeline with Streamlit ğŸ“„")
st.markdown("""
ì´ ì•±ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  AIë¥¼ í†µí•´ ì§ˆë¬¸ì— ë‹µë³€ì„ ìƒì„±í•˜ëŠ” RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.  
ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!
""")

# Sidebar ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    # OpenAI API í‚¤ ì…ë ¥
    api_key = st.text_input("OpenAI API Key", type="password")
    st.write("---")
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.txt, .pdf, .docx)", type=["txt", "pdf", "docx"])
    # GitHub ë§í¬
    github_link = "https://github.com/data-ai-insight/gpt-challenge.git"
    st.markdown(f"[ğŸ”— GitHub ë¦¬í¬ì§€í† ë¦¬]({github_link})")

# API í‚¤ ì…ë ¥ í™•ì¸
if not api_key:
    st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

# íŒŒì¼ ì—…ë¡œë“œ í™•ì¸ ë° ì²˜ë¦¬
if uploaded_file:
    st.info("íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    retriever = process_file(uploaded_file, api_key)
    chain = setup_chain(retriever, api_key)
    st.success("íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”.")

    # ì±„íŒ… ì„¸ì…˜ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_input = st.chat_input("íŒŒì¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”...")
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.chat_message("user").markdown(user_input)
        st.session_state["messages"].append({"role": "user", "content": user_input})

        # AI ë‹µë³€ ìƒì„±
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = chain.invoke({"question": user_input})
            answer = response["answer"]

        # AI ë‹µë³€ í‘œì‹œ
        st.chat_message("assistant").markdown(answer)
        st.session_state["messages"].append({"role": "assistant", "content": answer})
else:
    st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ AIì™€ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
